{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN for Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.11.2 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZXsd3cr1_c4",
        "outputId": "556976e8-a35b-46ab-ecab-c66df38fee75"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.11.2\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 11.6 MB/s \n",
            "\u001b[?25hCollecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.4 MB/s eta 0:00:36tcmalloc: large alloc 1147494400 bytes == 0x3a5ca000 @  0x7f580f0bd615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SMYqZpD-5Nq5"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n"
      ]
    },
    {
      "metadata": {
        "id": "HfXXxdem5SAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfe7479-b88b-4594-8682-607432bf1327"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Pytorch version is: \", torch.__version__)\n",
        "print(\"You are using: \", device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version is:  1.10.2+cu102\n",
            "You are using:  cuda\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sBT9IRQA5C7s"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Recurrent Sequence to Sequence Model\n"
      ]
    },
    {
      "metadata": {
        "id": "GQhdzxa1l2DQ"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Preparing Data\n"
      ]
    },
    {
      "metadata": {
        "id": "Yv5oe1W_WTn8"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "# import spacy\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.enabled = False \n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqyPgqMJWjEE"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! python -m spacy download en\n",
        "! python -m spacy download de\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9UnSP-kwWToK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027d176c-4e03-4407-8275-b84fb8bd98b2"
      },
      "cell_type": "code",
      "source": [
        "# get train test split using Multi30k \n",
        "train_data, valid_data, test_data = Multi30k()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.21M/1.21M [00:00<00:00, 5.84MB/s]\n",
            "100%|██████████| 46.3k/46.3k [00:00<00:00, 1.97MB/s]\n",
            "100%|██████████| 43.9k/43.9k [00:00<00:00, 1.98MB/s]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FhPbupsZWToO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc746e0-bb59-4078-f696-3dbfbf88d946"
      },
      "cell_type": "code",
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example testing \n",
        "example_src, example_trg = next(Multi30k(split=\"train\"))\n",
        "print(f\"Original: {example_src}\", f\"Tokenized: {de_tokenizer(example_src)}\")\n",
        "print(f\"Original: {example_trg}\", f\"Tokenized: {en_tokenizer(example_trg)}\")"
      ],
      "metadata": {
        "id": "ga395mn1biDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90939685-d7e1-4d6c-82cc-b27b48f97171"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            " Tokenized: ['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.', '\\n']\n",
            "Original: Two young, White males are outside near many bushes.\n",
            " Tokenized: ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cJ_1_SrKWToX"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the vocabulary for the source and target languages \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "syHVQKH1WToY"
      },
      "cell_type": "code",
      "source": [
        "# Only allow tokens that appear at least 2 times to appear in our vocabulary. \n",
        "# Tokens that appear only once are converted into an `<unk>` (unknown) token. \n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "de_generator = (de_tokenizer(pair[0].strip().lower()) for pair in Multi30k(split=\"train\"))\n",
        "specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "de_vocab = build_vocab_from_iterator(de_generator, specials=specials, min_freq=2)\n",
        "en_generator = (en_tokenizer(pair[1].strip().lower()) for pair in Multi30k(split=\"train\"))\n",
        "en_vocab = build_vocab_from_iterator(en_generator, specials=specials, min_freq=2)\n",
        "\n",
        "for vocab in (de_vocab, en_vocab):\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NBc-q3nAWToZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5e6441-6e9d-4a37-c792-656db04a6d70"
      },
      "cell_type": "code",
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Data Preprocessing Pipeline.\n"
      ],
      "metadata": {
        "id": "5C6pNVTxQnN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from typing import List, Tuple\n",
        "from torch import Tensor\n",
        "\n",
        "def data_process(raw_dataset) -> List[Tuple[Tensor, Tensor]]: \n",
        "    ret = []\n",
        "    for pair in raw_dataset: \n",
        "      # lower case and strip both German and English tokens in each sentence pair \n",
        "      d_tokens = de_tokenizer(pair[0].strip().lower()) \n",
        "      e_tokens = de_tokenizer(pair[1].strip().lower()) \n",
        "\n",
        "      # reverse sentence order of German tokens \n",
        "      d_tokens = d_tokens[::-1]\n",
        "\n",
        "      # add <bos> and <eos> tokens to both German and English sentences \n",
        "      d_tokens.insert(0, '<bos>')\n",
        "      d_tokens.append('<eos>')\n",
        "      e_tokens.insert(0, '<bos>')\n",
        "      e_tokens.append('<eos>') \n",
        "\n",
        "      # get encoded tensor tuple from vocabs  \n",
        "      d_tens = torch.tensor([de_vocab[token] for token in d_tokens], dtype=torch.long)\n",
        "      e_tens = torch.tensor([en_vocab[token] for token in e_tokens], dtype=torch.long)\n",
        "      tup = (d_tens, e_tens)\n",
        "\n",
        "      # add tensor tuple to list \n",
        "      ret.append(tup) \n",
        "\n",
        "    return ret \n",
        "\n",
        "train_data, valid_data, test_data = Multi30k()\n",
        "train_data_processed = data_process(train_data)\n",
        "valid_data_processed = data_process(valid_data)\n",
        "test_data_processed = data_process(test_data) "
      ],
      "metadata": {
        "id": "lhEPLkwbskZS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example testing \n",
        "de_itos = de_vocab.get_itos()\n",
        "en_itos = en_vocab.get_itos()\n",
        "de_encoded, en_encoded = train_data_processed[0]\n",
        "print(\" \".join([de_itos[item] for item in de_encoded]))\n",
        "print(\" \".join([en_itos[item] for item in en_encoded]))"
      ],
      "metadata": {
        "id": "AeSlO-aNICWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d95097-f646-41f3-9800-c24c8fffd4fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos> . büsche vieler nähe der in freien im sind männer weiße junge zwei <eos>\n",
            "<bos> two young , white males are outside near many bushes . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "\n",
        "def collate_fn(data_batch) -> Tuple[Tensor, List[int], Tensor]: \n",
        "    # initialize lists to be added to returned tuple \n",
        "    de_batch = [] \n",
        "    en_batch = [] \n",
        "    sent_lens = [] \n",
        "    for pair in data_batch: \n",
        "      de = pair[0]\n",
        "      en = pair[1] \n",
        "      # add indices/lengths to corresponding batches/list \n",
        "      de_batch.append(de)\n",
        "      en_batch.append(en)\n",
        "      sent_lens.append(len(de))\n",
        "    # pad sequences \n",
        "    # send to gpu \n",
        "    de_batch = torch.tensor(pad_sequence(de_batch, padding_value=PAD_IDX)).to(device) \n",
        "    en_batch = torch.tensor(pad_sequence(en_batch, padding_value=PAD_IDX)).to(device) \n",
        "    ret = (de_batch, sent_lens, en_batch)\n",
        "\n",
        "    return ret \n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_data_processed,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    valid_data_processed,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_data_processed,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "XthR0vAjvbUz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing collate_fn \n",
        "indices = [0, 1, 2, 3] \n",
        "collate_fn([train_data_processed[i] for i in indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW_Acz4ci8wK",
        "outputId": "5ec28160-9307-4b3f-a7e2-23ac86f23ff1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   2,    2,    2,    2],\n",
              "         [   4,    4,    4,    4],\n",
              "         [3171,    0,  499,  248],\n",
              "         [7649,    5,   56,    5],\n",
              "         [ 110, 2069, 7316,  681],\n",
              "         [  15,  831,    5,   10],\n",
              "         [   7,   11,    7,  535],\n",
              "         [  88,   30,  217,   14],\n",
              "         [  20,   76,   25,   12],\n",
              "         [  84,    3,   66,   29],\n",
              "         [  30,    1,    5,   40],\n",
              "         [ 253,    1,    3,   46],\n",
              "         [  26,    1,    1,    6],\n",
              "         [  18,    1,    1,    7],\n",
              "         [   3,    1,    1,   13],\n",
              "         [   1,    1,    1,    5],\n",
              "         [   1,    1,    1,    3]], device='cuda:0'),\n",
              " [15, 10, 12, 17],\n",
              " tensor([[   2,    2,    2,    2],\n",
              "         [  16,  113,    4,    4],\n",
              "         [  24,   30,   53,    9],\n",
              "         [  15,    6,   33,    6],\n",
              "         [  25,  325,  230,    4],\n",
              "         [ 778,  279,   69,   29],\n",
              "         [  17,   17,    4,   23],\n",
              "         [  57, 1200,  248,   10],\n",
              "         [  80,    4, 4286,   36],\n",
              "         [ 202,  715,    5,    8],\n",
              "         [1312, 4317,    3,    4],\n",
              "         [   5, 2879,    1,  576],\n",
              "         [   3,    5,    1,  574],\n",
              "         [   1,    3,    1,    4],\n",
              "         [   1,    1,    1,  240],\n",
              "         [   1,    1,    1,    5],\n",
              "         [   1,    1,    1,    3]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "WoL3rBhRWTog"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq RNN Model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6fz_08hMWToi"
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "  \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len, enforce_sorted=False)\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)       \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "        hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NST9l0-nWTon"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oc9aLkc2WTop"
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers)\n",
        "        self.out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(0) \n",
        "        \n",
        "        embedded = self.embedding(input) \n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden) \n",
        "        \n",
        "        prediction = self.out(output.squeeze(0))\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZc3dpkdWTos"
      },
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5VULJi93WTot"
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.enc_hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5): \n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        _, hidden = self.encoder(src, src_len)\n",
        "        hidden = hidden.unsqueeze(0)\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, max_len): \n",
        "            output, hidden = self.decoder(input, hidden)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = (trg[t] if teacher_force else top1)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "08P-oKpVWTov"
      },
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Model Training\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hqrUQvp6WTow"
      },
      "cell_type": "code",
      "source": [
        "# specify params \n",
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsLNEteqWToz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10f27dd-be6c-489c-b7b3-5fb9a2bf269a"
      },
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 10,616,326 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1XzEJnqxWTo3"
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bqkXf5ZNWTo7"
      },
      "cell_type": "code",
      "source": [
        "PAD_IDX = en_vocab['<pad>']\n",
        "assert PAD_IDX == de_vocab['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1xp_-5hWTpA"
      },
      "cell_type": "code",
      "source": [
        "# training loop \n",
        "def train(model, train_dl, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train() \n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, (src, src_len, trg) in enumerate(train_dl):\n",
        "        \n",
        "        optimizer.zero_grad() \n",
        "        output = model(src, src_len, trg) \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "    \n",
        "        trg = trg[1:].view(-1)\n",
        "       \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(train_dl)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oAZGOivbWTpF"
      },
      "cell_type": "code",
      "source": [
        "# evaluation \n",
        "def evaluate(model, val_dl, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, (src, src_len, trg) in enumerate(val_dl):\n",
        "\n",
        "            output = model(src, src_len, trg, 0)  \n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1) \n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(val_dl)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqThqiAwWTpI"
      },
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eTrSPLDDWTpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e73f3d1-977e-4684-9a96-9c89d14d6c9e"
      },
      "cell_type": "code",
      "source": [
        "# main training loop \n",
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "SAVE_DIR = 'models'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'cpsc477_hw4_rnn.pt')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# save model to reduce future runtime \n",
        "if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "    os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_dl, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_dl, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 57s\n",
            "\tTrain Loss: 4.372 | Train PPL:  79.173\n",
            "\t Val. Loss: 4.180 |  Val. PPL:  65.342\n",
            "Epoch: 02 | Time: 0m 57s\n",
            "\tTrain Loss: 3.487 | Train PPL:  32.695\n",
            "\t Val. Loss: 3.922 |  Val. PPL:  50.516\n",
            "Epoch: 03 | Time: 0m 58s\n",
            "\tTrain Loss: 3.143 | Train PPL:  23.169\n",
            "\t Val. Loss: 3.735 |  Val. PPL:  41.904\n",
            "Epoch: 04 | Time: 0m 57s\n",
            "\tTrain Loss: 2.841 | Train PPL:  17.134\n",
            "\t Val. Loss: 3.743 |  Val. PPL:  42.212\n",
            "Epoch: 05 | Time: 0m 57s\n",
            "\tTrain Loss: 2.641 | Train PPL:  14.030\n",
            "\t Val. Loss: 3.761 |  Val. PPL:  42.973\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "I2UNcaY-WTpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952be2cb-abb7-4876-a38e-54e9a420577d"
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "test_loss = evaluate(model, test_dl, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 3.740 | Test PPL:  42.081 |\n"
          ]
        }
      ]
    }
  ]
}